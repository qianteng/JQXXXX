
**Feature engineering:**
* TripType: the label that we need to predict, use label encoder to convert into a 0->38 ranged int
* Weekday: convert to int (0->6) and use one hot to convert to 7 variables w/ 0 or 1. (there's no linear relationship btw week days)
* ScanCount: iterms purchased, I aggregated it on a per VisitNumber , DepartmentDescription, FinlineNumber level 
* FinanlineNumber (fl) and DepartmentDescription (dd): I created another variable as fldd (fl and dd combined)
* training data matrics: 
* aggregated to VisitNumber level, so each VN now only have 1 row, but multiple columns for different fldd. 
* to speed up and help debug, I OH encoded only the fldd w/ at least 100 transaction records, now only limits to 1300 fldd categories * which covers 80% of all training data. not bad. 
* stored as sparse metrics. 

**Parameter Turning:**
* have not tried systematic tuning (such as grid search or randomized cv). only manually tried a few choices of eta and max tree depth (depth of 10 seems to be a good choice)  


**emsembling:**
* No emsembling done so far as I only just got xgb to work correctly. 


**result**
* score is 2.2, seems a bit off compared to others who only rely on xgb. probably have to use a full-bown sparse matrix that encode all the "rare" fldd. 


**TODO**
* was just getting xgb to work properly, haven't really dive into top rank's solutions, next step will be 
  * 1)include all the rare fldd 
  * 2)improve feature engineering and use ideas from top performers


