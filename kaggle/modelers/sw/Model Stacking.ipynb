{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected Models for stacking are: random forest tree, KerasCNN, XGboost, logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# wrapper classes for Xgboost\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# BaseEstimator provides among other things a default implementation for the get_params and set_params methods\n",
    "# It helps the pipline and feature union at the stacking step\n",
    "class XgboostForStacking(BaseEstimator):\n",
    "      def __int__(self, seed, numOfThreads, numOfRounds,\n",
    "                  eta, gamma\n",
    "                  depth, min_child_weight, max_delta,\n",
    "                  subsample, colsample_bytree, silent, \n",
    "                  l2_reg, l1_reg):\n",
    "            \n",
    "        self.seed = seed\n",
    "        self.numOfThreads = numOfThreads\n",
    "        self.numOfRounds = numOfRounds\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.depth = depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.max_delta = max_delta\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.silent = silent\n",
    "        self.l2_reg = l2_reg\n",
    "        self.l1_reg = l1_reg\n",
    "        \n",
    "        self.n_class = None\n",
    "        self.model = None\n",
    "        self.param = {}\n",
    "        # use softmax multi-class classification\n",
    "        self.param['objective'] = 'multi:softmax'\n",
    "        # scale weight of positive examples\n",
    "        self.param['eta'] = eta\n",
    "        self.param['max_depth'] = depth\n",
    "        self.param['silent'] = silent\n",
    "        self.param['nthread'] = numOfThreads\n",
    "        self.param['gamma'] = gamma\n",
    "        self.param['alpha'] = l1_reg\n",
    "        self.param['lambda'] = l2_reg\n",
    "        self.parm['seed'] = seed\n",
    "        self.param[\"min_child_weight\"] = min_child_weight\n",
    "        self.param[\"max_delta_step\"] = max_delta_step\n",
    "        self.param[\"subsample\"] = subsample\n",
    "        self.param[\"silent\"] = silent\n",
    "        self.param[\"colsample_bytree\"] = colsample_bytree\n",
    "       \n",
    "    '''\n",
    "    x, y are pandas dataframe \n",
    "    '''\n",
    "    def fit(self, x, y):\n",
    "        # encoding the labels in y\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        y = encoder.fit_transform(y)\n",
    "        self.n_class = np.unique(y).shape[0] # number of classes in unique y\n",
    "        self.param['num_classes'] = self.n_class\n",
    "        xgtrain = xgb.DMatrix(x.values, y.values)\n",
    "        self.model = xgb.train(params, xgtrain, self.numOfRounds)\n",
    "      \n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        x_test = xgb.DMatrix(x.values)\n",
    "        pred = self.model.predcit(x_test)\n",
    "        return pred\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from Keras network classfier backened by theano\n",
    "class NeuralNetworkClassifier(BaseEstimator):\n",
    "      def __init__(self, batch, hidden_units, hidden_layers, input_dropout, hidden_dropout, \n",
    "                   prelu, hidden_activation, batch_size, nb_epoch, optimizer, learning_rate,\n",
    "                   momentum, decay, rho, epsilon, validation_split):\n",
    "        self.batch = batch\n",
    "        self.hidden_units = hidden_units\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.input_dropout = input_dropout\n",
    "        self.prelu = prelu\n",
    "        self.hidden_dropout = hidden_dropout \n",
    "        self.hidden_activation = hidden_activation\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.decay = decay\n",
    "        self.rho = rho\n",
    "        self.epsilon = self.epsilon\n",
    "        self.validation_split = validation_split\n",
    "        self.model = None\n",
    "       \n",
    "       def fit(self, X, y):\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            y = le.fit_transform(y).astype(np.int32)\n",
    "            y =  np_utils.to_categorical(y)\n",
    "            self.model = Sequrntial()\n",
    "            # hidden layers\n",
    "            first = True\n",
    "            hidden_layers = self.hidden_layers\n",
    "            while hidden_layers > 0:\n",
    "                if first:\n",
    "                    dim = X.shape[1]\n",
    "                    first = False\n",
    "                else:\n",
    "                    dim = self.hidden_units\n",
    "                self.model.add(Dense(dim, self.hidden_units, init = \"uniform\"))\n",
    "                if self.batch_norm:\n",
    "                    self.model.add(BatchNormalization((self.hidden_units,)))\n",
    "                if self.prelu == True:\n",
    "                    self.model.add(PReLU((self.hidden_units,)))\n",
    "                else:\n",
    "                    self.model.add(Activation(self.hidden_activation))\n",
    "                self.model.add(Dropout(self.hidden_dropout))\n",
    "                hidden_layers -= 1\n",
    "                \n",
    "            # Output layer\n",
    "            self.model.add(Dense(self.hidden_units, nb_classes, init = \"uniform\"))\n",
    "            self.model.add(Activation('softmax'))                    \n",
    "    \n",
    "            # Optimizer\n",
    "            if self.optimizer == \"adam\":\n",
    "            adam = keras.optimizers.Adam(self.learning_rate, beta_1=0.9, beta_2=0.999, epsilon=self.epsilon)\n",
    "            self.model.compile(loss='categorical_crossentropy', optimizer=adam)\n",
    "                \n",
    "            self.model.fit(X, y, nb_epoch=self.nb_epoch, batch_size=self.batch_size,\n",
    "                      validation_split=self.validation_split)\n",
    "        return self   \n",
    "    \n",
    "        def predict(self, X):\n",
    "            pred = self.model_predict_proba(X)\n",
    "            return pred    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
